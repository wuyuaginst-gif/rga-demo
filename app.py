import os
from typing import List, Dict
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.schema.document import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.chains import RetrievalQA

# --- é…ç½®å‚æ•° ---
# ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸” Qwen2:7b æ¨¡å‹å·²æ‹‰å–
OLLAMA_MODEL = "qwen2:7b" 
VECTOR_DB_PATH = "./chroma_db_requirements"
COLLECTION_NAME = "hubei_nongxin_requirements"
BASE_URL = "http://localhost:11434" # Ollama é»˜è®¤åœ°å€

# --- 1. æ¨¡æ‹Ÿå†å²éœ€æ±‚æ•°æ® ---
# æ¨¡æ‹Ÿæ‚¨ä»å†å² Excel æˆ–ç³»ç»Ÿå¯¼å‡ºçš„æ•°æ®
HISTORICAL_REQUIREMENTS: List[Dict[str, str]] = [
    {
        "id": "XQ-20230101-001",
        "title": "ç½‘é“¶æ¸ é“æ–°å¢è½¬è´¦æ±‡æ¬¾åŠŸèƒ½ï¼Œæ”¯æŒå¤§é¢å’Œå®šæ—¶äº¤æ˜“ã€‚",
        "content": "ä¸šåŠ¡éƒ¨è¦æ±‚åœ¨ä¸ªäººç½‘é“¶ä¸­å¢åŠ æ¯æ—¥è¶…è¿‡50ä¸‡å…ƒçš„è½¬è´¦äº¤æ˜“åŠŸèƒ½ï¼Œå¹¶æä¾›é¢„è®¾æ—¶é—´è½¬è´¦ã€‚",
        "solution": "å·²å¼€å‘å®Œæˆï¼Œä½¿ç”¨äº†ç¬¬ä¸‰æ–¹å®‰å…¨æ¨¡å—è¿›è¡ŒåŠ å¯†ã€‚",
        "status": "å·²ä¸Šçº¿"
    },
    {
        "id": "XQ-20230315-002",
        "title": "æŸœé¢ç³»ç»Ÿä¼˜åŒ–ï¼Œæé«˜å­˜å–æ¬¾æ•ˆç‡ã€‚",
        "content": "æŸœé¢æ“ä½œäººå‘˜åé¦ˆï¼Œå­˜å–æ¬¾æµç¨‹æ­¥éª¤è¿‡å¤šï¼Œå¸Œæœ›æ•´åˆåˆ°å•é¡µé¢ï¼Œå‡å°‘ç‚¹å‡»ã€‚",
        "solution": "ä¼˜åŒ–äº†å‰ç«¯ç•Œé¢ï¼Œå°†å¤šä¸ªæ­¥éª¤åˆå¹¶ï¼Œå‡å°‘äº†å“åº”æ—¶é—´ã€‚",
        "status": "å·²ä¸Šçº¿"
    },
    {
        "id": "XQ-20240520-003",
        "title": "ç§»åŠ¨Appæ”¯æŒç”Ÿç‰©è¯†åˆ«ç™»å½•å’Œå¿«æ·æ”¯ä»˜ã€‚",
        "content": "ç§‘æŠ€éƒ¨å»ºè®®åœ¨Appä¸­å¼•å…¥æŒ‡çº¹å’Œäººè„¸è¯†åˆ«ï¼Œå¹¶æ”¯æŒå°é¢å…å¯†æ”¯ä»˜ï¼Œæé«˜ç”¨æˆ·ä½“éªŒã€‚",
        "solution": "æ­£åœ¨å¼€å‘ä¸­ï¼Œé¢„è®¡2025å¹´Q1æŠ•äº§ã€‚",
        "status": "å¼€å‘ä¸­"
    },
    {
        "id": "XQ-20230102-004",
        "title": "ç½‘é“¶æ¸ é“ä¼˜åŒ–ï¼Œæ”¯æŒå¤§é¢æ±‡æ¬¾å’Œé¢„çº¦åŠŸèƒ½ã€‚",
        "content": "é›¶å”®ä¸šåŠ¡éƒ¨æå‡ºï¼Œå®¢æˆ·éœ€è¦é¢„çº¦ç‰¹å®šæ—¥æœŸå’Œé‡‘é¢çš„æ±‡æ¬¾ï¼Œä½†ç°æœ‰ç½‘é“¶ç³»ç»Ÿä¸æ”¯æŒã€‚",
        "solution": "å·²åœ¨2023å¹´Q2å®ç°ï¼ŒåŠŸèƒ½ä¸001å·éœ€æ±‚ç±»ä¼¼ï¼Œä½†ä¾§é‡é¢„çº¦ã€‚",
        "status": "å·²ä¸Šçº¿"
    }
]

# --- 2. æ„å»ºçŸ¥è¯†åº“ (Embedding & Store) ---

def build_knowledge_base():
    """å°†å†å²éœ€æ±‚æ•°æ®å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° ChromaDB"""
    
    print("ğŸš€ æ­¥éª¤ 1: åˆå§‹åŒ– Ollama Embeddings æ¨¡å‹...")
    # ä½¿ç”¨ Ollama çš„ API ä½œä¸ºåµŒå…¥æ¨¡å‹ (é»˜è®¤ä½¿ç”¨ 'llama2'ï¼Œä½†æ€§èƒ½å–å†³äºæ¨¡å‹)
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ¨èä½¿ç”¨ä¸“é—¨çš„æœ¬åœ°ä¸­æ–‡ Embedding æ¨¡å‹
    embeddings = OllamaEmbeddings(model=OLLAMA_MODEL, base_url=BASE_URL)

    # å°† Python Dict è½¬æ¢ä¸º LangChain Document æ ¼å¼
    documents = []
    for req in HISTORICAL_REQUIREMENTS:
        # ä½¿ç”¨æ ‡é¢˜+å†…å®¹ä½œä¸º Document çš„ page_contentï¼Œå…ƒæ•°æ®å­˜å‚¨è¯¦ç»†ä¿¡æ¯
        content = f"éœ€æ±‚æ ‡é¢˜: {req['title']}\néœ€æ±‚å†…å®¹: {req['content']}"
        doc = Document(page_content=content, metadata=req)
        documents.append(doc)

    print(f"ğŸ“– æ­¥éª¤ 2: æ­£åœ¨åˆ›å»º/åŠ è½½å‘é‡æ•°æ®åº“åˆ°è·¯å¾„: {VECTOR_DB_PATH}")
    # åˆ›å»º ChromaDB å‘é‡å­˜å‚¨ï¼Œå¹¶å¯¼å…¥ Document
    vectorstore = Chroma.from_documents(
        documents=documents, 
        embedding=embeddings, 
        persist_directory=VECTOR_DB_PATH,
        collection_name=COLLECTION_NAME
    )
    vectorstore.persist()
    print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…±è®¡ %d æ¡éœ€æ±‚ã€‚" % len(documents))
    return vectorstore

# --- 3. æ™ºèƒ½æŸ¥é‡ä¸åˆ†æ ---

def smart_deduplication_analysis(vectorstore: Chroma, new_requirement: str):
    """
    æ¥æ”¶æ–°éœ€æ±‚ï¼Œè¿›è¡Œå‘é‡æœç´¢å’Œ LLM åˆ†æã€‚
    :param new_requirement: æ–°éœ€æ±‚çš„æ ‡é¢˜å’Œå†…å®¹
    """
    print("\nğŸ” æ­¥éª¤ 3: æ­£åœ¨å¯¹æ–°éœ€æ±‚è¿›è¡Œæ™ºèƒ½æŸ¥é‡åˆ†æ...")
    
    # åˆå§‹åŒ–æœ¬åœ° LLM
    llm = Ollama(model=OLLAMA_MODEL, base_url=BASE_URL)

    # LangChain Prompt Template - å®ç°æŸ¥é‡åˆ†æ
    # ä½¿ç”¨ä¸­æ–‡æç¤ºè¯ï¼Œå¼•å¯¼ LLM è¿›è¡Œè§’è‰²æ‰®æ¼”å’Œç»“æ„åŒ–è¾“å‡º
    prompt_template = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é“¶è¡Œç§‘æŠ€éƒ¨é—¨é¡¹ç›®ç»ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯å®¡æ ¸æ–°çš„ä¸šåŠ¡éœ€æ±‚ï¼Œå¹¶åˆ¤æ–­å®ƒæ˜¯å¦ä¸å†å²éœ€æ±‚é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼ã€‚
    
    ã€å†å²ç›¸ä¼¼éœ€æ±‚å‚è€ƒã€‘ï¼š
    {context}

    ã€å½“å‰æäº¤çš„æ–°éœ€æ±‚ã€‘ï¼š
    {question}

    è¯·æ ¹æ®å†å²å‚è€ƒï¼Œç»™å‡ºä½ çš„ä¸“ä¸šå»ºè®®ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤éœ€æ±‚ã€‚
    è¯·ä»¥æ¸…æ™°çš„åˆ†ç‚¹æ ¼å¼è¾“å‡ºï¼š
    1. æŸ¥é‡ç»“è®ºï¼ˆæ˜¯/å¦é‡å¤ï¼‰ï¼š
    2. ç›¸ä¼¼åº¦æœ€é«˜çš„å†å²éœ€æ±‚IDå’Œæ ‡é¢˜ï¼š
    3. è¯¦ç»†åˆ†æå’Œå»ºè®®ï¼ˆä¾‹å¦‚ï¼šç›¸ä¼¼åº¦85%ï¼Œå»ºè®®åˆå¹¶è‡³XQ-20230101-001é¡¹ç›®çš„äºŒæœŸè¿›è¡Œå¼€å‘ï¼‰ï¼š
    """

    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # åˆ›å»º RetrievalQA é“¾
    # retriver ä¼šè‡ªåŠ¨æ ¹æ®æ–°éœ€æ±‚åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢æœ€ç›¸ä¼¼çš„ K ä¸ªæ–‡æ¡£
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å¡å…¥ä¸Šä¸‹æ–‡
        retriever=vectorstore.as_retriever(search_kwargs={"k": 2}), # æœç´¢æœ€ç›¸ä¼¼çš„2æ¡
        chain_type_kwargs={"prompt": PROMPT}
    )

    # è¿è¡ŒæŸ¥é‡åˆ†æé“¾
    result = qa_chain.invoke(new_requirement)
    
    print("\n--- AI æŸ¥é‡åˆ†æç»“æœ ---")
    print(result['result'])
    print("------------------------")


# --- ä¸»ç¨‹åºæ‰§è¡Œé€»è¾‘ ---

if __name__ == "__main__":
    # 1. ç¡®ä¿çŸ¥è¯†åº“å·²æ„å»º
    vector_db = build_knowledge_base()
    
    # 2. æ¨¡æ‹Ÿä¸€ä¸ªæ–°çš„éœ€æ±‚æäº¤ï¼ˆä¸ XQ-20230101-001/XQ-20230102-004 é«˜åº¦ç›¸ä¼¼ï¼‰
    NEW_REQUEST_1 = "ä¸šåŠ¡éƒ¨é—¨è¦æ±‚åœ¨æ‰‹æœºé“¶è¡ŒAppä¸Šå¢åŠ å¤§é¢è½¬è´¦åŠŸèƒ½ï¼Œå¹¶æä¾›é¢„çº¦è½¬è´¦çš„é€‰é¡¹ã€‚"
    
    smart_deduplication_analysis(vector_db, NEW_REQUEST_1)
    
    print("\n\n" + "="*50 + "\n")
    
    # 3. æ¨¡æ‹Ÿå¦ä¸€ä¸ªæ–°çš„éœ€æ±‚æäº¤ï¼ˆä¸ç›¸ä¼¼ï¼‰
    NEW_REQUEST_2 = "éœ€è¦è°ƒæ•´å†…éƒ¨äººåŠ›èµ„æºç³»ç»Ÿçš„æƒé™é…ç½®ï¼Œå¢åŠ ä¸€ä¸ªâ€œä¸´æ—¶ç®¡ç†å‘˜â€è§’è‰²ã€‚"
    
    smart_deduplication_analysis(vector_db, NEW_REQUEST_2)
